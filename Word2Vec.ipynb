{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4LSwA9LO8j9",
    "outputId": "4f00db84-a70c-4d8c-cf97-a75b7bf50c56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ritar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk, itertools, requests, zipfile, os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import product\n",
    "from time import time \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxitmu2mkdkp"
   },
   "source": [
    "## Download e descompactação do corpus disponibilizado pelo Professor: [text8](http://mattmahoney.net/dc/text8.zip)\n",
    "\n",
    "O download do link foi realizado pela biblioteca **requests**, para a descompactação do arquivo zip foi utilizada a biblioteca **zipfile**. Por fim o conteudo do arquivo é lido e armazenado em uma variável *corpus* como uma cadeia de caractéres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term \n"
     ]
    }
   ],
   "source": [
    "def get_corpus(path, file_name, url):\n",
    "    \n",
    "    if not os.path.isfile(path + 'text8'):\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            open(path + file_name , 'wb').write(r.content)\n",
    "            with zipfile.ZipFile(path + file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(path)\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError, \n",
    "                requests.exceptions.Timeout, requests.exceptions.RequestException) as err:\n",
    "            print (\"Error Connecting:\",err)  \n",
    "            return None\n",
    "\n",
    "    with open(path + 'text8', 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    \n",
    "    return data\n",
    "\n",
    "corpus = get_corpus('./data/', 'text8.zip', 'http://mattmahoney.net/dc/text8.zip')\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download de analogias disponibilizadas pelo Professor: [analogias](https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt)\n",
    "O download do link foi realizado pela biblioteca **requests**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Athens Greece Baghdad Iraq', 'Athens Greece Bangkok Thailand', 'Athens Greece Beijing China', 'Athens Greece Berlin Germany', 'Athens Greece Bern Switzerland', 'Athens Greece Cairo Egypt', 'Athens Greece Canberra Australia', 'Athens Greece Hanoi Vietnam', 'Athens Greece Havana Cuba', 'Athens Greece Helsinki Finland']\n"
     ]
    }
   ],
   "source": [
    "def get_analogies(path, file_name, url):\n",
    "\n",
    "    if not os.path.isfile(path + file_name):\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            open(path + file_name , 'wb').write(r.content)\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.HTTPError, \n",
    "                requests.exceptions.Timeout, requests.exceptions.RequestException) as err:\n",
    "            print (\"Error Connecting:\",err)  \n",
    "            return None\n",
    "\n",
    "    data = []\n",
    "    with open(path + file_name) as fp:\n",
    "        lines = fp.readlines()\n",
    "        lines.pop(0)\n",
    "        for line in lines:\n",
    "            data.append(line.strip())\n",
    "    \n",
    "    return data\n",
    "\n",
    "analogies = get_analogies('./data/', 'questions-words.txt', 'https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt')\n",
    "print(analogies[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY4QrcTJPnF_"
   },
   "source": [
    "## Pré-Processamento de Dados\n",
    "Nesta etapa, todo o texto é transformado numa lista de palavras e colocado em caixa baixa. Além disso, pontuações, números, caractéres especiais e stopwords foram removidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrxKfQ90utCg"
   },
   "source": [
    "#### Texto convertido para caixa baixa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAuJ_NdFqZzM",
    "outputId": "313a8a30-77c7-40df-ce7c-74678c6b3026"
   },
   "outputs": [],
   "source": [
    "corpus = corpus.lower()\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWQIah1iuzW7"
   },
   "source": [
    "#### Texto transformado em uma lista de palavras utilizando a biblioteca **NLTK**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4Sgo6Hll08F",
    "outputId": "47efda13-edae-4743-8854-e1561eadfd95"
   },
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(corpus)\n",
    "print(word_tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlFTqMQfvKh5"
   },
   "source": [
    "#### Stopwords, obtidas pelo pacote **NLTK**, foram retiradas do texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLzepQjzPIyM",
    "outputId": "2dd8ef1b-59b9-4c69-e92b-f39b6b4e0973"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [[w for w in word_tokens if not w in stop_words]]\n",
    "print(filtered_words[0][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mXMX5b_sEyb"
   },
   "source": [
    "## Treinamento dos Modelos\n",
    "Em seguida por intermédio da biblioteca gensim os modelos *Continuous Bag of Words* e *Skip-Gram* foram treinados. Como hiperparâmetros dos modelos é interessante destacar os seguintes atributos:\n",
    "#### Parâmetros fixos\n",
    "\n",
    "*   epochs = 10\n",
    "*   min_count = 2\n",
    "*   report_delay = 1\n",
    "*   alpha(learning_rate) = 0.01\n",
    "\n",
    "#### Parâmetros que foram variados\n",
    "*   vector_size: define a dimensão dos embeddings\n",
    "*   window: define o tamanho do contexto a ser analizado, ou seja a distância máxima entre a palavra atual e a palavra predita.\n",
    "*   total_words: Quantidade de palavras utilizadas no treinamento  \n",
    "*   sg: O algoritmo de treinamento, pode ser tanto o *Continuous Bag of Words*, codificado como 0, ou o *Skip-Gram*, codificado como 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vector_size, window, total_words):\n",
    "    return [list(i) for i in product(vector_size, window, total_words)]\n",
    "\n",
    "params = get_params(vector_size = [50, 100, 300], window = [3, 6, 10], \n",
    "           total_words=[int(len(filtered_words[0])/4), int(len(filtered_words[0])/2), int(len(filtered_words[0]))])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de treinamento dos modelos:\n",
    "Recebe como parâmetro a dimensão dos embeddings, a janela (ou seja a distância de palavras analisadas), a quantidade de palavras analisadas e o algoritmo (*Skip-Gram* ou *Continuous Bag of Words*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkF1YC2Br8cm"
   },
   "outputs": [],
   "source": [
    "def train_model(vector_size, window, total_words, algorithm_name):\n",
    "    \n",
    "    algorithm = 1 if \"skipgram\" else 0\n",
    "    file_name = \"./models/{}/vector_size_{}_window_{}_total_words_{}.sav\".format(algorithm_name, vector_size, window, total_words)\n",
    "    \n",
    "    if os.path.isfile(file_name):\n",
    "        print(\"Modelo com os parâmetros passados já existe em \", file_name)\n",
    "    else:\n",
    "        print(\"Modelo com os seguintes parâmetros será treinado:\")\n",
    "        print(\"Dimensão do vetor: {} \\nWindow: {} \\nQuantidade de Palavras: {}\\n\".format(vector_size, window, total_words))\n",
    "        \n",
    "        t = time()\n",
    "        skip_gram = Word2Vec(filtered_words, vector_size=vector_size, sg=algorithm, window=window, min_count=2, alpha=0.01)\n",
    "        print('Tempo para construir vocabulario: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        \n",
    "        t = time()\n",
    "        skip_gram.train(filtered_words, total_examples = total_words, epochs=12, report_delay=1)\n",
    "        print('Tempo para treinar o modelo: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "        skip_gram.save(file_name)\n",
    "        print(\"Model saved in\", file_name)\n",
    "        \n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jWDmetkuyMt"
   },
   "source": [
    "### Treinamento do *Skip-Gram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcoAnVUXGWqm",
    "outputId": "ce2a8ac7-1c87-4d96-e74c-5e72fd04b605"
   },
   "outputs": [],
   "source": [
    "for combinations in params:\n",
    "    train_model(vector_size=combinations[0], window=combinations[1], total_words=combinations[2], algorithm_name=\"skipgram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do *Continuous Bag of Words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWJvgcqW9Djp",
    "outputId": "a63284d6-b382-4027-af99-beb408897eb2"
   },
   "outputs": [],
   "source": [
    "for combinations in params:\n",
    "    train_model(vector_size=combinations[0], window=combinations[1], total_words=combinations[2], algorithm_name=\"cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMzGSB+En+Iujr8v+I24WcV",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1-ShORRcjh1fhRQX7CIkEGxueFQG5Czp8",
   "name": "Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
